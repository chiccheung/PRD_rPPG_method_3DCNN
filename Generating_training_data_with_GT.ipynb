{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyVHR.datasets.ubfc2 import UBFC2\n",
    "from pyVHR.datasets.dataset import Dataset\n",
    "from pyVHR.datasets.dataset import datasetFactory\n",
    "from pyVHR.methods.base import methodFactory\n",
    "from pyVHR.signals.video import Video\n",
    "from copy import copy\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "#RUN ON CPU \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "#Tensorflow/KERAS\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.python.keras.models import model_from_json\n",
    "from tensorflow.python.keras.layers import ZeroPadding3D, Dense, Activation,Conv3D,MaxPooling3D,AveragePooling3D,Flatten,Dropout\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "from tensorflow.python.keras.models import model_from_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- dataset object\n",
    "dataset = UBFC2()\n",
    "\n",
    "# -- ground-truth (GT) signal\n",
    "fname = \"./UBFC/DATASET_2/subject1/ground_truth.txt\"\n",
    "\n",
    "# -- load signal and build a BVPsignal or ECGsignal object\n",
    "sigGT = dataset.readSigfile(fname)\n",
    "\n",
    "# -- compute BPM GT\n",
    "winSizeGT = 2\n",
    "bpmGT, timesGT = sigGT.getBPM(winSizeGT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2. 12. 16. 15. 18. 16. 18. 14. 18. 18. 22. 20. 26. 16. 18. 20. 20. 19.\n",
      " 22. 23. 24. 20. 18. 22. 26. 19. 18. 20. 15. 19. 21. 23. 24. 21. 22. 18.\n",
      " 22. 17. 23. 17. 24. 23. 25. 20. 15. 20. 18. 21. 23. 22. 20. 22. 19. 21.\n",
      " 19.]\n"
     ]
    }
   ],
   "source": [
    "bpm = np.round(bpmGT)\n",
    "bpm =bpm - 55\n",
    "bpm = np.round(bpm / 2.5)\n",
    "print(bpm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
      " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
      " 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53.\n",
      " 54.]\n"
     ]
    }
   ],
   "source": [
    "print(timesGT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LENGTH_VIDEO = 60 \n",
    "IMAGE_WIDTH = 25 \n",
    "IMAGE_HEIGHT = 25 \n",
    "IMAGE_CHANNELS = 1 \n",
    "RATE = 30\n",
    "NB_SECOND = int(LENGTH_VIDEO / RATE)\n",
    "# Available Outputs\n",
    "HEART_RATES = np.linspace(55, 240, 75)\n",
    "NB_CLASSES = len(HEART_RATES)\n",
    "\n",
    "\n",
    "def extractDataFromVideo(videoFilename, GTFilename):\n",
    "    \n",
    "    sigGT = dataset.readSigfile(GTFilename)\n",
    "    winSizeGT = NB_SECOND\n",
    "    bpmGT, timesGT = sigGT.getBPM(winSizeGT)\n",
    "    \n",
    "    bpm = np.round(bpmGT)\n",
    "    bpm = bpm - 55\n",
    "    bpm = np.round(bpm / 2.5)\n",
    "    \n",
    "    video = Video(videoFilename)\n",
    "    video.getCroppedFaces(detector='dlib', extractor='skvideo')\n",
    "    video.setMask(typeROI='skin_adapt',skinThresh_adapt=0.22)\n",
    "\n",
    "    NB_LAPSE = int(video.numFrames / RATE)\n",
    "\n",
    "    imgs = np.zeros(shape=(video.numFrames, video.cropSize[0], video.cropSize[1], 1))\n",
    "    xtest = np.zeros(shape=(NB_LAPSE, LENGTH_VIDEO, IMAGE_HEIGHT , IMAGE_WIDTH, 1))\n",
    "    ytest = np.zeros(shape=(NB_LAPSE, NB_CLASSES + 1))\n",
    "\n",
    "    # prepare labels and label categories\n",
    "    labels = np.zeros(NB_CLASSES + 1)\n",
    "\n",
    "    for i in range(NB_CLASSES + 1):\n",
    "        labels[i] = i\n",
    "    labels_cat = np_utils.to_categorical(labels)\n",
    " \n",
    "    # channel extraction\n",
    "    if (video.cropSize[2]<3):\n",
    "        IMAGE_CHANNELS = 1\n",
    "    else:\n",
    "        IMAGE_CHANNELS = video.cropSize[2]\n",
    "\n",
    "    # load images (imgs contains the whole video)\n",
    "    for j in range(video.numFrames):\n",
    "\n",
    "        if (IMAGE_CHANNELS==3):\n",
    "            temp = video.faces[j]/255\n",
    "            temp = temp[:,:,1]      # only the G component is currently used\n",
    "        else:\n",
    "            temp = video.faces[j] / 255\n",
    "\n",
    "        imgs[j] = np.expand_dims(temp, 2)\n",
    "    \n",
    "\n",
    "\n",
    "    for lapse in range(0,NB_LAPSE):  \n",
    "        xtemp = np.zeros(shape=(LENGTH_VIDEO, IMAGE_HEIGHT , IMAGE_WIDTH, 1)) \n",
    "    \n",
    "        start = lapse * RATE\n",
    "        end = start + LENGTH_VIDEO\n",
    "        if(end > video.numFrames):\n",
    "            break\n",
    "        c=0\n",
    "    \n",
    "        for j in range(start,end-1):    \n",
    "            faceCopy = copy(imgs[j])\n",
    "            #randomization pixel choice \n",
    "            np.random.shuffle(faceCopy)\n",
    "            for m in range(0, IMAGE_HEIGHT):\n",
    "                for n in range(0, IMAGE_WIDTH):\n",
    "                    xtemp[c][m][n]= faceCopy[m][n]\n",
    "            c = c +1\n",
    "                \n",
    "        xtest[lapse] = np.expand_dims(xtemp, 0)\n",
    "        ytest[lapse] = np.expand_dims(labels_cat[int(bpm[lapse+NB_SECOND])],0)\n",
    "    return xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1629, 60, 25, 25, 1)\n"
     ]
    }
   ],
   "source": [
    "dataset = datasetFactory(\"UBFC2\")\n",
    "\n",
    "xtrain = np.array(np.zeros(shape=(0,LENGTH_VIDEO, IMAGE_HEIGHT, IMAGE_WIDTH, 1)))\n",
    "ytrain = np.zeros(shape=(0, NB_CLASSES + 1))\n",
    "\n",
    "for i in range (len(dataset.videoFilenames)):\n",
    "    xtest, ytest = extractDataFromVideo(dataset.videoFilenames[i], dataset.sigFilenames[i])\n",
    "    xtrain = np.concatenate((xtrain, xtest), axis=0)\n",
    "    ytrain = np.concatenate((ytrain, ytest), axis=0)\n",
    "\n",
    "indices = np.arange(xtrain.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "xtrain = xtrain[indices]\n",
    "ytrain = ytrain[indices]\n",
    "\n",
    "np.savez('./trainingData.npz', a=xtrain, b=ytrain)\n",
    "print(np.shape(xtrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7/7 [==============================] - 32s 5s/step - loss: 1390.7048 - accuracy: 0.0153\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 32s 5s/step - loss: 613.5331 - accuracy: 0.0362\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 32s 5s/step - loss: 336.6241 - accuracy: 0.0270\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 32s 5s/step - loss: 204.1564 - accuracy: 0.0313\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 32s 5s/step - loss: 135.8026 - accuracy: 0.0368\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 32s 5s/step - loss: 101.0680 - accuracy: 0.0417\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 34s 5s/step - loss: 75.9226 - accuracy: 0.0405\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 32s 5s/step - loss: 58.0394 - accuracy: 0.0399\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 33s 5s/step - loss: 41.6665 - accuracy: 0.0307\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 38s 5s/step - loss: 30.8884 - accuracy: 0.0270\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x170192e4e10>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model_from_json(open('./model/model_conv3D.json').read())\n",
    "model.load_weights('./model/weights_conv3D.h5')\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#start training\n",
    "model.fit(xtrain, ytrain, epochs = 10, batch_size=256, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7/7 [==============================] - 35s 5s/step - loss: 5.7733 - accuracy: 0.0405\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 43s 6s/step - loss: 4.2402 - accuracy: 0.0540\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 37s 5s/step - loss: 4.2166 - accuracy: 0.0565\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 41s 6s/step - loss: 4.1854 - accuracy: 0.0546\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 33s 5s/step - loss: 4.1447 - accuracy: 0.0675\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 33s 5s/step - loss: 4.0925 - accuracy: 0.0620\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 33s 5s/step - loss: 4.0292 - accuracy: 0.0546\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 33s 5s/step - loss: 3.9565 - accuracy: 0.0645\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 33s 5s/step - loss: 3.8758 - accuracy: 0.0528\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 33s 5s/step - loss: 3.7953 - accuracy: 0.0675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17019bb2588>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEFINE MODEL\n",
    "model = Sequential()\n",
    "\n",
    "#feature extraction\n",
    "model.add(Conv3D(filters=32, kernel_size=(LENGTH_VIDEO-2,IMAGE_HEIGHT-5,IMAGE_WIDTH-5), input_shape=(LENGTH_VIDEO, IMAGE_HEIGHT, IMAGE_WIDTH, 1)))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "#Classification\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(NB_CLASSES + 1, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#start training\n",
    "model.fit(xtrain, ytrain, epochs = 10, batch_size=256, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
