{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This jupyter notebook aims to create a formatted machine learning dataset from rPPG video datasets. Then, to notice the effectiveness of a machine learning model training on this dataset. The only treatment performed on the videos is a face extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Librairies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PyVHR Framework\n",
    "from pyVHR.datasets.ubfc2 import UBFC2\n",
    "from pyVHR.datasets.dataset import Dataset\n",
    "from pyVHR.datasets.dataset import datasetFactory\n",
    "from pyVHR.methods.base import methodFactory\n",
    "from pyVHR.signals.video import Video\n",
    "\n",
    "\n",
    "#Tensorflow/KERAS\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.python.keras.models import model_from_json\n",
    "from tensorflow.python.keras.layers import ZeroPadding3D, Dense, Activation,Conv3D,MaxPooling3D,AveragePooling3D,Flatten,Dropout\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "from tensorflow.python.keras.models import model_from_json\n",
    "\n",
    "# Copy / numpy / OpenCV\n",
    "from copy import copy\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protocol for transforming a video into a machine learning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LENGTH_VIDEO = 60 \n",
    "IMAGE_WIDTH = 25 \n",
    "IMAGE_HEIGHT = 25 \n",
    "IMAGE_CHANNELS = 1 \n",
    "RATE = 30\n",
    "NB_SECOND = int(LENGTH_VIDEO / RATE)\n",
    "# Available Outputs\n",
    "HEART_RATES = np.linspace(55, 240, 75)\n",
    "NB_CLASSES = len(HEART_RATES)\n",
    "\n",
    "\n",
    "def extractDataFromVideo(videoFilename, GTFilename):\n",
    "    \n",
    "    sigGT = dataset.readSigfile(GTFilename)\n",
    "    winSizeGT = NB_SECOND\n",
    "    bpmGT, timesGT = sigGT.getBPM(winSizeGT)\n",
    "    \n",
    "    # Format the GT\n",
    "    bpm = np.round(bpmGT)\n",
    "    bpm = bpm - 55\n",
    "    bpm = np.round(bpm / 2.5)\n",
    "    \n",
    "    #extraction\n",
    "    video = Video(videoFilename)\n",
    "    video.getCroppedFaces(detector='dlib', extractor='skvideo')\n",
    "    video.setMask(typeROI='skin_adapt',skinThresh_adapt=0.22)\n",
    "\n",
    "    NB_LAPSE = int(video.numFrames / RATE)\n",
    "\n",
    "    imgs = np.zeros(shape=(video.numFrames, video.cropSize[0], video.cropSize[1], 1))\n",
    "    xtest = np.zeros(shape=(NB_LAPSE, LENGTH_VIDEO, IMAGE_HEIGHT , IMAGE_WIDTH, 1))\n",
    "    ytest = np.zeros(shape=(NB_LAPSE, NB_CLASSES + 1))\n",
    "\n",
    "    # prepare labels and label categories\n",
    "    labels = np.zeros(NB_CLASSES + 1)\n",
    "\n",
    "    for i in range(NB_CLASSES + 1):\n",
    "        labels[i] = i\n",
    "    labels_cat = np_utils.to_categorical(labels)\n",
    " \n",
    "    # channel extraction\n",
    "    if (video.cropSize[2]<3):\n",
    "        IMAGE_CHANNELS = 1\n",
    "    else:\n",
    "        IMAGE_CHANNELS = video.cropSize[2]\n",
    "\n",
    "    # load images (imgs contains the whole video)\n",
    "    for j in range(video.numFrames):\n",
    "\n",
    "        if (IMAGE_CHANNELS==3):\n",
    "            temp = video.faces[j]/255\n",
    "            temp = temp[:,:,1]      # only the G component is currently used\n",
    "        else:\n",
    "            temp = video.faces[j] / 255\n",
    "\n",
    "        imgs[j] = np.expand_dims(temp, 2)\n",
    "    \n",
    "\n",
    "    # Construction of sequences for each time interval\n",
    "    for lapse in range(0,NB_LAPSE):  \n",
    "        xtemp = np.zeros(shape=(LENGTH_VIDEO, IMAGE_HEIGHT , IMAGE_WIDTH, 1)) \n",
    "    \n",
    "        start = lapse * RATE\n",
    "        end = start + LENGTH_VIDEO\n",
    "        if(end > video.numFrames):\n",
    "            break\n",
    "        c=0\n",
    "    \n",
    "        for j in range(start,end-1):    \n",
    "            faceCopy = copy(imgs[j])\n",
    "            faceCopy = cv2.resize(faceCopy,(IMAGE_HEIGHT , IMAGE_WIDTH))\n",
    "            faceCopy = faceCopy - np.mean(faceCopy)\n",
    "            for m in range(0, IMAGE_HEIGHT):\n",
    "                for n in range(0, IMAGE_WIDTH):\n",
    "                    xtemp[c][m][n]= faceCopy[m][n]\n",
    "            c = c +1\n",
    "        #Sequence        \n",
    "        xtest[lapse] = np.expand_dims(xtemp, 0)\n",
    "        #GT\n",
    "        ytest[lapse] = np.expand_dims(labels_cat[int(bpm[lapse+NB_SECOND])],0)\n",
    "    return xtest, ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Applying the transformation on UBFC2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\florian\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pyVHR\\signals\\video.py:71: UserWarning:\n",
      "\n",
      "\n",
      "WARNING!! Requested detector method is different from the saved one\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1629, 60, 25, 25, 1)\n"
     ]
    }
   ],
   "source": [
    "dataset = datasetFactory(\"UBFC2\")\n",
    "\n",
    "xtrain = np.array(np.zeros(shape=(0,LENGTH_VIDEO, IMAGE_HEIGHT, IMAGE_WIDTH, 1)))\n",
    "ytrain = np.zeros(shape=(0, NB_CLASSES + 1))\n",
    "\n",
    "# For each video in the dataset\n",
    "for i in range (len(dataset.videoFilenames)):\n",
    "    xtest, ytest = extractDataFromVideo(dataset.videoFilenames[i], dataset.sigFilenames[i])\n",
    "    xtrain = np.concatenate((xtrain, xtest), axis=0)\n",
    "    ytrain = np.concatenate((ytrain, ytest), axis=0)\n",
    "\n",
    "# Mix the sequences\n",
    "indices = np.arange(xtrain.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "xtrain = xtrain[indices]\n",
    "ytrain = ytrain[indices]\n",
    "\n",
    "# save\n",
    "np.savez('./data.npz', a=xtrain, b=ytrain)\n",
    "print(np.shape(xtrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Division into 1 test dataset and 1 validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1466, 60, 25, 25, 1)\n",
      "(1466, 76)\n",
      "(163, 60, 25, 25, 1)\n",
      "(163, 76)\n"
     ]
    }
   ],
   "source": [
    "data = np.load('./data.npz')\n",
    "# 90% -> test & 10% -> validation\n",
    "pct = 0.9\n",
    "sizeDataset = data['a'].shape[0]\n",
    "sizeTrainData = int(sizeDataset * pct) \n",
    "\n",
    "xtrain = data['a'][:sizeTrainData,:]\n",
    "xvalidation = data['a'][sizeTrainData:,:]\n",
    "\n",
    "ytrain = data['b'][:sizeTrainData,:]\n",
    "yvalidation = data['b'][sizeTrainData:,:]\n",
    "\n",
    "np.savez('./dataSplited.npz', a=xtrain, b=ytrain, c=xvalidation, d=yvalidation)\n",
    "\n",
    "print(np.shape(xtrain))\n",
    "print(np.shape(ytrain))\n",
    "print(np.shape(xvalidation))\n",
    "print(np.shape(yvalidation))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing datasets on a model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "92/92 [==============================] - 36s 389ms/step - loss: 3.3564 - accuracy: 0.0921\n",
      "Epoch 2/20\n",
      "92/92 [==============================] - 36s 387ms/step - loss: 2.8609 - accuracy: 0.1405\n",
      "Epoch 3/20\n",
      "92/92 [==============================] - 35s 382ms/step - loss: 2.6942 - accuracy: 0.1480\n",
      "Epoch 4/20\n",
      "92/92 [==============================] - 36s 395ms/step - loss: 2.6192 - accuracy: 0.1746\n",
      "Epoch 5/20\n",
      "92/92 [==============================] - 35s 383ms/step - loss: 2.5294 - accuracy: 0.1774\n",
      "Epoch 6/20\n",
      "92/92 [==============================] - 35s 384ms/step - loss: 2.4799 - accuracy: 0.1971\n",
      "Epoch 7/20\n",
      "92/92 [==============================] - 38s 408ms/step - loss: 2.4012 - accuracy: 0.1944\n",
      "Epoch 8/20\n",
      "92/92 [==============================] - 36s 391ms/step - loss: 2.3742 - accuracy: 0.1971\n",
      "Epoch 9/20\n",
      "92/92 [==============================] - 35s 383ms/step - loss: 2.3430 - accuracy: 0.2087\n",
      "Epoch 10/20\n",
      "92/92 [==============================] - 35s 383ms/step - loss: 2.2815 - accuracy: 0.2265\n",
      "Epoch 11/20\n",
      "92/92 [==============================] - 35s 383ms/step - loss: 2.2590 - accuracy: 0.2456\n",
      "Epoch 12/20\n",
      "92/92 [==============================] - 36s 387ms/step - loss: 2.2293 - accuracy: 0.2271\n",
      "Epoch 13/20\n",
      "92/92 [==============================] - 35s 385ms/step - loss: 2.2195 - accuracy: 0.2415\n",
      "Epoch 14/20\n",
      "92/92 [==============================] - 36s 387ms/step - loss: 2.1723 - accuracy: 0.2469\n",
      "Epoch 15/20\n",
      "92/92 [==============================] - 36s 394ms/step - loss: 2.1007 - accuracy: 0.2804\n",
      "Epoch 16/20\n",
      "92/92 [==============================] - 35s 382ms/step - loss: 2.1508 - accuracy: 0.2456\n",
      "Epoch 17/20\n",
      "92/92 [==============================] - 36s 391ms/step - loss: 2.1065 - accuracy: 0.2688\n",
      "Epoch 18/20\n",
      "92/92 [==============================] - 35s 381ms/step - loss: 2.0976 - accuracy: 0.2763\n",
      "Epoch 19/20\n",
      "92/92 [==============================] - 35s 383ms/step - loss: 2.1006 - accuracy: 0.2701\n",
      "Epoch 20/20\n",
      "92/92 [==============================] - 37s 404ms/step - loss: 2.0866 - accuracy: 0.2885\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fb2daba780>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEFINE MODEL\n",
    "model = Sequential()\n",
    "\n",
    "#feature extraction\n",
    "model.add(Conv3D(filters=32, kernel_size=(LENGTH_VIDEO-2,IMAGE_HEIGHT-5,IMAGE_WIDTH-5), input_shape=(LENGTH_VIDEO, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS)))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "#Classification\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(NB_CLASSES + 1, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(xtrain, ytrain, epochs = 20, batch_size=16, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can learn from this data despite its small number. Indeed, we can see that the accuracy is higher than 7%, so the model is more efficient than chance. Therefore, it makes sense to enrich our synthetic data set with this data set to hopefully improve the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
