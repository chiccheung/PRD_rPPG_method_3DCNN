{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This jupyter notebook aims to create a formatted machine learning dataset from rPPG video datasets. Then, to notice the effectiveness of a machine learning model training on this dataset. The only treatment performed on the videos is a face extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Librairies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PyVHR Framework\n",
    "from pyVHR.datasets.ubfc2 import UBFC2\n",
    "from pyVHR.datasets.dataset import Dataset\n",
    "from pyVHR.datasets.dataset import datasetFactory\n",
    "from pyVHR.methods.base import methodFactory\n",
    "from pyVHR.signals.video import Video\n",
    "\n",
    "\n",
    "#Tensorflow/KERAS\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.python.keras.models import model_from_json\n",
    "from tensorflow.python.keras.layers import ZeroPadding3D, Dense, Activation,Conv3D,MaxPooling3D,AveragePooling3D,Flatten,Dropout\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "from tensorflow.python.keras.models import model_from_json\n",
    "\n",
    "# Copy / numpy / OpenCV\n",
    "from copy import copy\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protocol for transforming a video into a machine learning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LENGTH_VIDEO = 60 \n",
    "IMAGE_WIDTH = 25 \n",
    "IMAGE_HEIGHT = 25 \n",
    "IMAGE_CHANNELS = 1 \n",
    "RATE = 30\n",
    "NB_SECOND = int(LENGTH_VIDEO / RATE)\n",
    "# Available Outputs\n",
    "HEART_RATES = np.linspace(55, 240, 75)\n",
    "NB_CLASSES = len(HEART_RATES)\n",
    "\n",
    "\n",
    "def extractDataFromVideo(videoFilename, GTFilename):\n",
    "    \n",
    "    sigGT = dataset.readSigfile(GTFilename)\n",
    "    winSizeGT = NB_SECOND\n",
    "    bpmGT, timesGT = sigGT.getBPM(winSizeGT)\n",
    "    \n",
    "    # Format the GT\n",
    "    bpm = np.round(bpmGT)\n",
    "    bpm = bpm - 55\n",
    "    bpm = np.round(bpm / 2.5)\n",
    "    \n",
    "    #extraction\n",
    "    video = Video(videoFilename)\n",
    "    video.getCroppedFaces(detector='dlib', extractor='skvideo')\n",
    "    video.setMask(typeROI='skin_adapt',skinThresh_adapt=0.22)\n",
    "\n",
    "    NB_LAPSE = int(video.numFrames / RATE)\n",
    "\n",
    "    imgs = np.zeros(shape=(video.numFrames, video.cropSize[0], video.cropSize[1], 1))\n",
    "    xtest = np.zeros(shape=(NB_LAPSE, LENGTH_VIDEO, IMAGE_HEIGHT , IMAGE_WIDTH, 1))\n",
    "    ytest = np.zeros(shape=(NB_LAPSE, NB_CLASSES + 1))\n",
    "\n",
    "    # prepare labels and label categories\n",
    "    labels = np.zeros(NB_CLASSES + 1)\n",
    "\n",
    "    for i in range(NB_CLASSES + 1):\n",
    "        labels[i] = i\n",
    "    labels_cat = np_utils.to_categorical(labels)\n",
    " \n",
    "    # channel extraction\n",
    "    if (video.cropSize[2]<3):\n",
    "        IMAGE_CHANNELS = 1\n",
    "    else:\n",
    "        IMAGE_CHANNELS = video.cropSize[2]\n",
    "\n",
    "    # load images (imgs contains the whole video)\n",
    "    for j in range(video.numFrames):\n",
    "\n",
    "        if (IMAGE_CHANNELS==3):\n",
    "            temp = video.faces[j]/255\n",
    "            temp = temp[:,:,1]      # only the G component is currently used\n",
    "        else:\n",
    "            temp = video.faces[j] / 255\n",
    "\n",
    "        imgs[j] = np.expand_dims(temp, 2)\n",
    "    \n",
    "\n",
    "    # Construction of sequences for each time interval\n",
    "    for lapse in range(0,NB_LAPSE):  \n",
    "        xtemp = np.zeros(shape=(LENGTH_VIDEO, IMAGE_HEIGHT , IMAGE_WIDTH, 1)) \n",
    "    \n",
    "        start = lapse * RATE\n",
    "        end = start + LENGTH_VIDEO\n",
    "        if(end > video.numFrames):\n",
    "            break\n",
    "        c=0\n",
    "    \n",
    "        for j in range(start,end-1):    \n",
    "            faceCopy = copy(imgs[j])\n",
    "            faceCopy = cv2.resize(faceCopy,(IMAGE_HEIGHT , IMAGE_WIDTH))\n",
    "            for m in range(0, IMAGE_HEIGHT):\n",
    "                for n in range(0, IMAGE_WIDTH):\n",
    "                    xtemp[c][m][n]= faceCopy[m][n]\n",
    "            c = c +1\n",
    "        #Sequence        \n",
    "        xtest[lapse] = np.expand_dims(xtemp, 0)\n",
    "        #GT\n",
    "        ytest[lapse] = np.expand_dims(labels_cat[int(bpm[lapse+NB_SECOND])],0)\n",
    "    return xtest, ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Applying the transformation on UBFC2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1629, 60, 25, 25, 1)\n"
     ]
    }
   ],
   "source": [
    "dataset = datasetFactory(\"UBFC2\")\n",
    "\n",
    "xtrain = np.array(np.zeros(shape=(0,LENGTH_VIDEO, IMAGE_HEIGHT, IMAGE_WIDTH, 1)))\n",
    "ytrain = np.zeros(shape=(0, NB_CLASSES + 1))\n",
    "\n",
    "# For each video in the dataset\n",
    "for i in range (len(dataset.videoFilenames)):\n",
    "    xtest, ytest = extractDataFromVideo(dataset.videoFilenames[i], dataset.sigFilenames[i])\n",
    "    xtrain = np.concatenate((xtrain, xtest), axis=0)\n",
    "    ytrain = np.concatenate((ytrain, ytest), axis=0)\n",
    "\n",
    "# Mix the sequences\n",
    "indices = np.arange(xtrain.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "xtrain = xtrain[indices]\n",
    "ytrain = ytrain[indices]\n",
    "\n",
    "# save\n",
    "np.savez('./data.npz', a=xtrain, b=ytrain)\n",
    "print(np.shape(xtrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Division into 1 test dataset and 1 validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1466, 60, 25, 25, 1)\n",
      "(1466, 76)\n",
      "(163, 60, 25, 25, 1)\n",
      "(163, 76)\n"
     ]
    }
   ],
   "source": [
    "data = np.load('./data.npz')\n",
    "# 90% -> test & 10% -> validation\n",
    "pct = 0.9\n",
    "sizeDataset = data['a'].shape[0]\n",
    "sizeTrainData = int(sizeDataset * pct) \n",
    "\n",
    "xtrain = data['a'][:sizeTrainData,:]\n",
    "xvalidation = data['a'][sizeTrainData:,:]\n",
    "\n",
    "ytrain = data['b'][:sizeTrainData,:]\n",
    "yvalidation = data['b'][sizeTrainData:,:]\n",
    "\n",
    "np.savez('./dataSplited.npz', a=xtrain, b=ytrain, c=xvalidation, d=yvalidation)\n",
    "\n",
    "print(np.shape(xtrain))\n",
    "print(np.shape(ytrain))\n",
    "print(np.shape(xvalidation))\n",
    "print(np.shape(yvalidation))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing datasets on a model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "92/92 [==============================] - 36s 390ms/step - loss: 4.2883 - accuracy: 0.0532\n",
      "Epoch 2/40\n",
      "92/92 [==============================] - 35s 378ms/step - loss: 3.5456 - accuracy: 0.0607\n",
      "Epoch 3/40\n",
      "92/92 [==============================] - 35s 379ms/step - loss: 3.4046 - accuracy: 0.0641\n",
      "Epoch 4/40\n",
      "92/92 [==============================] - 35s 377ms/step - loss: 3.3767 - accuracy: 0.0621\n",
      "Epoch 5/40\n",
      "92/92 [==============================] - 35s 377ms/step - loss: 3.3655 - accuracy: 0.0757\n",
      "Epoch 6/40\n",
      "92/92 [==============================] - 35s 382ms/step - loss: 3.3626 - accuracy: 0.0709\n",
      "Epoch 7/40\n",
      "92/92 [==============================] - 35s 380ms/step - loss: 3.3611 - accuracy: 0.0696\n",
      "Epoch 8/40\n",
      "92/92 [==============================] - 36s 391ms/step - loss: 3.3630 - accuracy: 0.0689\n",
      "Epoch 9/40\n",
      "92/92 [==============================] - 35s 379ms/step - loss: 3.3744 - accuracy: 0.0730\n",
      "Epoch 10/40\n",
      "92/92 [==============================] - 35s 383ms/step - loss: 3.3650 - accuracy: 0.0628\n",
      "Epoch 11/40\n",
      "92/92 [==============================] - 35s 379ms/step - loss: 3.3900 - accuracy: 0.0696\n",
      "Epoch 12/40\n",
      "92/92 [==============================] - 36s 394ms/step - loss: 3.4204 - accuracy: 0.0566\n",
      "Epoch 13/40\n",
      "92/92 [==============================] - 35s 383ms/step - loss: 3.4188 - accuracy: 0.0662\n",
      "Epoch 14/40\n",
      "92/92 [==============================] - 35s 379ms/step - loss: 3.4246 - accuracy: 0.0675\n",
      "Epoch 15/40\n",
      "92/92 [==============================] - 35s 378ms/step - loss: 3.4576 - accuracy: 0.0573\n",
      "Epoch 16/40\n",
      "92/92 [==============================] - 35s 378ms/step - loss: 3.4632 - accuracy: 0.0566\n",
      "Epoch 17/40\n",
      "92/92 [==============================] - 35s 381ms/step - loss: 3.5041 - accuracy: 0.0703\n",
      "Epoch 18/40\n",
      "92/92 [==============================] - 35s 382ms/step - loss: 3.5312 - accuracy: 0.0600\n",
      "Epoch 19/40\n",
      "92/92 [==============================] - 35s 378ms/step - loss: 3.5889 - accuracy: 0.0634\n",
      "Epoch 20/40\n",
      "92/92 [==============================] - 35s 379ms/step - loss: 3.5970 - accuracy: 0.0730\n",
      "Epoch 21/40\n",
      "92/92 [==============================] - 35s 377ms/step - loss: 3.6131 - accuracy: 0.0628\n",
      "Epoch 22/40\n",
      "92/92 [==============================] - 35s 381ms/step - loss: 3.6403 - accuracy: 0.0628\n",
      "Epoch 23/40\n",
      "92/92 [==============================] - 35s 384ms/step - loss: 3.6498 - accuracy: 0.0600\n",
      "Epoch 24/40\n",
      "92/92 [==============================] - 35s 380ms/step - loss: 3.7554 - accuracy: 0.0539\n",
      "Epoch 25/40\n",
      "92/92 [==============================] - 35s 382ms/step - loss: 3.7337 - accuracy: 0.0696\n",
      "Epoch 26/40\n",
      "92/92 [==============================] - 35s 379ms/step - loss: 3.7709 - accuracy: 0.0716\n",
      "Epoch 27/40\n",
      "92/92 [==============================] - 35s 386ms/step - loss: 3.8691 - accuracy: 0.0525\n",
      "Epoch 28/40\n",
      "92/92 [==============================] - 35s 382ms/step - loss: 3.9908 - accuracy: 0.0600\n",
      "Epoch 29/40\n",
      "92/92 [==============================] - 35s 380ms/step - loss: 3.7854 - accuracy: 0.0593\n",
      "Epoch 30/40\n",
      "92/92 [==============================] - 35s 381ms/step - loss: 3.9012 - accuracy: 0.0682\n",
      "Epoch 31/40\n",
      "92/92 [==============================] - 35s 380ms/step - loss: 3.9986 - accuracy: 0.0628\n",
      "Epoch 32/40\n",
      "92/92 [==============================] - 35s 381ms/step - loss: 3.7716 - accuracy: 0.0655\n",
      "Epoch 33/40\n",
      "92/92 [==============================] - 35s 380ms/step - loss: 4.2189 - accuracy: 0.0641\n",
      "Epoch 34/40\n",
      "92/92 [==============================] - 35s 381ms/step - loss: 4.1059 - accuracy: 0.0546\n",
      "Epoch 35/40\n",
      "92/92 [==============================] - 35s 381ms/step - loss: 3.9338 - accuracy: 0.0621\n",
      "Epoch 36/40\n",
      "92/92 [==============================] - 35s 381ms/step - loss: 4.2107 - accuracy: 0.0559\n",
      "Epoch 37/40\n",
      "92/92 [==============================] - 35s 378ms/step - loss: 4.3703 - accuracy: 0.0662\n",
      "Epoch 38/40\n",
      "92/92 [==============================] - 35s 378ms/step - loss: 4.3041 - accuracy: 0.0593\n",
      "Epoch 39/40\n",
      "92/92 [==============================] - 35s 380ms/step - loss: 4.4694 - accuracy: 0.0566\n",
      "Epoch 40/40\n",
      "92/92 [==============================] - 35s 386ms/step - loss: 4.3193 - accuracy: 0.0634\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x27ab50e4ac8>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEFINE MODEL\n",
    "model = Sequential()\n",
    "\n",
    "#feature extraction\n",
    "model.add(Conv3D(filters=32, kernel_size=(LENGTH_VIDEO-2,IMAGE_HEIGHT-5,IMAGE_WIDTH-5), input_shape=(LENGTH_VIDEO, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS)))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "#Classification\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(NB_CLASSES + 1, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(xtrain, ytrain, epochs = 40, batch_size=16, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model cannot learn. We can see that the precision is below 7 percent, so the model is as efficient as the hazard. With today's knowledge, it is therefore not a good idea to enrich our synthetic dataset with this dataset to improve the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
