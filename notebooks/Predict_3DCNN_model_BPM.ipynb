{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of a new rPPG method\n",
    "\n",
    "## Part 2 : Notebook for the prediction of the 3D-CNN model\n",
    "\n",
    "This jupyter notebook file complements the \"Train_3DCNN_model_BPM.ipynb\" file. In this file, we can test the model predictions on real videos and highlight logic of the future implementation into the pyVHR framework. ([Link](https://ieeexplore.ieee.org/document/9272290)) ([GitHub](https://github.com/phuselab/pyVHR))\n",
    "\n",
    "This file is based on the implementation described in the following article :\n",
    "Frédéric Bousefsaf, Alain Pruski, Choubeila Maaoui, 3D convolutional neural networks for remote pulse rate measurement and mapping from facial video, Applied Sciences, vol. 9, n° 20, 4364 (2019). ([Link](https://www.mdpi.com/2076-3417/9/20/4364)) ([GitHub](https://github.com/frederic-bousefsaf/ippg-3dcnn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries\n",
    "\n",
    "Previously , you have to install theses python librairies :\n",
    "* tensorflow\n",
    "* matplotlib\n",
    "* scipy\n",
    "* numpy\n",
    "* opencv-python\n",
    "* Copy\n",
    "* pyVHR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#RUN ON CPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "#Tensorflow/KERAS\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.models import model_from_json\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "\n",
    "# Numpy / Matplotlib / OpenCV / Scipy / Copy\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import scipy.stats as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from copy import copy\n",
    "\n",
    "#pyVHR\n",
    "from pyVHR.signals.video import Video\n",
    "from pyVHR.datasets.dataset import Dataset\n",
    "from pyVHR.datasets.dataset import datasetFactory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the video & pyVHR processing\n",
    "\n",
    "\n",
    "In the pyVHR framework, we work on a processed video. The processing consists of detecting and extracting an area of interest, in order to apply our rPPGs methods on relevant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Video object\n",
    "def extractionROI(videoFilename):\n",
    "    video = Video(videoFilename)\n",
    "    video.getCroppedFaces(detector='dlib', extractor='skvideo')\n",
    "    video.setMask(typeROI='skin_adapt',skinThresh_adapt=0.22)\n",
    "    return video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model\n",
    "Load model & classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "def loadmodel(MODEL_PATH):\n",
    "    model = model_from_json(open(f'{MODEL_PATH}/model_conv3D.json').read())\n",
    "    model.load_weights(f'{MODEL_PATH}/weights_conv3D.h5')\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # define the frequencies // output dimension (number of classes used during training)\n",
    "    freq_BPM = np.linspace(55, 240, num=model.output_shape[1]-1)\n",
    "    freq_BPM = np.append(freq_BPM, -1)     # noise class\n",
    "    return model, freq_BPM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting videoframes to a single channel array\n",
    "\n",
    "Select one channel for making prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. LOAD DATA\n",
    "def convertVideoToTable(video,model):\n",
    "    imgs = np.zeros(shape=(model.input_shape[1], video.cropSize[0], video.cropSize[1], 1))\n",
    "\n",
    "    # channel extraction\n",
    "    if (video.cropSize[2]<3):\n",
    "        IMAGE_CHANNELS = 1\n",
    "    else:\n",
    "        IMAGE_CHANNELS = video.cropSize[2]\n",
    "\n",
    "    # load images (imgs contains the whole video)\n",
    "    for j in range(model.input_shape[1]):\n",
    "\n",
    "        if (IMAGE_CHANNELS==3):\n",
    "            temp = video.faces[j]/255\n",
    "            temp = temp[:,:,1]      # only the G component is currently used\n",
    "        else:\n",
    "            temp = video.faces[j] / 255\n",
    "\n",
    "        imgs[j] = np.expand_dims(temp, 2)\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formating Video\n",
    "\n",
    "Create a list of frames from video and select representative pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatingDataTest(video, model, imgs):\n",
    "    xtest = np.zeros(shape=(model.input_shape[1], model.input_shape[2] , model.input_shape[3], 1))\n",
    "    for j in range(0,model.input_shape[1]):   # j = nb frames\n",
    "        faceCopy = copy(imgs[j])\n",
    "        faceCopy = faceCopy - np.mean(faceCopy)\n",
    "        faceCopy = cv2.resize(faceCopy,(model.input_shape[2], model.input_shape[3]))\n",
    "        for m in range(0, model.input_shape[2]):\n",
    "            for n in range(0, model.input_shape[3]):\n",
    "                xtest[j][m][n]= faceCopy[m][n]\n",
    "                \n",
    "    return xtest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a prediction\n",
    "\n",
    "Use the model to make prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPrediction(model,freq_BPM, xtest):\n",
    "    idx =0\n",
    "    maxi =0\n",
    "    # model.predict\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(xtest, 0))\n",
    "    h = model(input_tensor)\n",
    "    h = h.numpy() \n",
    "    #prediction\n",
    "    return h[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the label associated with the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClass(h, freq_BPM):\n",
    "    idx =0\n",
    "    maxi =0\n",
    "    #find label associated\n",
    "    for i in range(0, len(h)):\n",
    "        if maxi < h[i]:\n",
    "            idx = i\n",
    "            maxi = h[i]\n",
    "\n",
    "    return freq_BPM[idx]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a prediction\n",
    "\n",
    "Function to make prediction on veritable data (60 first frames only in this example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f902c3e2d74f83ab134ba6320f4d7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='frame', max=1533, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPM frequency estimated = 105.0\n"
     ]
    }
   ],
   "source": [
    "videoFilename = \"./UBFC/DATASET_2/subject1/vid.avi\"  #video to be processed path\n",
    "modelFilename = \"./model\"   #model path \n",
    "\n",
    "def makePrediction(videoFilename, modelFilename):\n",
    "    # ROI EXTRACTION\n",
    "    video = extractionROI(videoFilename)\n",
    "    # print ROI EXTRACTION\n",
    "    video.showVideo()  \n",
    "    #Load the model\n",
    "    model, freq_BPM = loadmodel(modelFilename)\n",
    "    #extract Green channel or Black & whrite channel\n",
    "    framesOneChannel = convertVideoToTable(video,model)\n",
    "    #Data preparation \n",
    "    xtest = formatingDataTest(video, model, framesOneChannel)\n",
    "    prediction = getPrediction(model,freq_BPM,xtest)\n",
    "    bpm = getClass(prediction, freq_BPM)\n",
    "    return bpm\n",
    "\n",
    "print('BPM frequency estimated = ' + str(makePrediction(videoFilename, modelFilename)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation test on veritable data\n",
    "\n",
    "Test on 60 first frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f41a6c3cf3204f728f43fd535be4914b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='frame', max=1986, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Video 1 : 67.5\n",
      "GT Video 0 : 67.5\n",
      "ABS DIFF Video 1 : 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed73327cec394899be98a4a756e7359b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='frame', max=2049, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Video 2 : 90.0\n",
      "GT Video 1 : 95.0\n",
      "ABS DIFF Video 2 : 5.0\n"
     ]
    }
   ],
   "source": [
    "videoFilenames = [\"./UBFC/DATASET_2/subject12/vid.avi\", \"./UBFC/DATASET_2/subject16/vid.avi\"]\n",
    "GT = [\"./UBFC/DATASET_2/subject12/ground_truth.txt\",\"./UBFC/DATASET_2/subject16/ground_truth.txt\"]\n",
    "\n",
    "modelFilename = \"./model\"\n",
    "model, freq_BPM = loadmodel(modelFilename)\n",
    "\n",
    "dataset = datasetFactory(\"UBFC2\")\n",
    "winSizeGT = 2      \n",
    "\n",
    "for i in range(0, len(videoFilenames)):\n",
    "    prediction = makePrediction(videoFilenames[i], modelFilename)\n",
    "    print(\"Prediction Video \"+ str(i+1) +\" : \"+ str(prediction))\n",
    "    \n",
    "    sigGT = dataset.readSigfile(GT[i])\n",
    "    bpmGT, timesGT = sigGT.getBPM(winSizeGT)\n",
    "    # Format the GT\n",
    "    bpm = np.round(bpmGT)\n",
    "    bpm = bpm - 55\n",
    "    bpm = np.round(bpm / 2.5)\n",
    "    GT_value = freq_BPM[int(bpm[2])]\n",
    "    print(\"GT Video \"+ str(i) +\" : \"+str(GT_value))\n",
    "    \n",
    "    print(\"ABS DIFF Video \"+ str(i+1) +\" : \"+str(abs(GT_value-prediction)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
