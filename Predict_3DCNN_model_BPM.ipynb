{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of a new rPPG method\n",
    "\n",
    "## Part 2 : Notebook for the prediction of the 3D-CNN model\n",
    "\n",
    "This jupyter notebook file complements the \"Train_3DCNN_model_BPM.ipynb\" file. In this file, we can test the model predictions on real videos and highlight logic of the future implementation into the pyVHR framework. ([Link](https://ieeexplore.ieee.org/document/9272290)) ([GitHub](https://github.com/phuselab/pyVHR))\n",
    "\n",
    "This file is based on the implementation described in the following article :\n",
    "Frédéric Bousefsaf, Alain Pruski, Choubeila Maaoui, 3D convolutional neural networks for remote pulse rate measurement and mapping from facial video, Applied Sciences, vol. 9, n° 20, 4364 (2019). ([Link](https://www.mdpi.com/2076-3417/9/20/4364)) ([GitHub](https://github.com/frederic-bousefsaf/ippg-3dcnn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import of libraries\n",
    "\n",
    "Previously , you have to install theses python librairies :\n",
    "* tensorflow\n",
    "* matplotlib\n",
    "* scipy\n",
    "* numpy\n",
    "* opencv-python\n",
    "* Copy\n",
    "* pyVHR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#RUN ON CPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "#Tensorflow/KERAS\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.models import model_from_json\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "\n",
    "# Numpy / Matplotlib / OpenCV / Scipy / Copy\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import scipy.stats as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from copy import copy\n",
    "\n",
    "#pyVHR\n",
    "from pyVHR.signals.video import Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load video\n",
    "\n",
    "\n",
    "In the pyVHR framework, we work on a processed video. The processing consists of detecting and extracting an area of interest, in order to apply our rPPGs methods on relevant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0bc8c36f44b4978ac535ea4344b322f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='frame', max=1533, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -- Video object\n",
    "videoFilename = \"./UBFC/DATASET_2/subject1/vid.avi\"\n",
    "video = Video(videoFilename)\n",
    "video.getCroppedFaces(detector='dlib', extractor='skvideo')\n",
    "video.setMask(typeROI='skin_adapt',skinThresh_adapt=0.22)\n",
    "video.showVideo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features of Frames\n",
    "IMAGE_HEIGHT = video.cropSize[0]\n",
    "IMAGE_WIDTH = video.cropSize[1]\n",
    "\n",
    "if (video.cropSize[2]<3):\n",
    "    IMAGE_CHANNELS = 1\n",
    "else:\n",
    "    IMAGE_CHANNELS = video.cropSize[2]\n",
    "\n",
    "# Load model\n",
    "RESULTS_PATH = \"./model\"\n",
    "model = model_from_json(open(f'{RESULTS_PATH}/model_conv3D.json').read())\n",
    "model.load_weights(f'{RESULTS_PATH}/weights_conv3D.h5')\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# define the frequencies // output dimension (number of classes used during training)\n",
    "freq_BPM = np.linspace(55, 240, num=model.output_shape[1]-1)\n",
    "freq_BPM = np.append(freq_BPM, -1)     # noise class\n",
    "\n",
    "# define patch size and number of images per video (directly from the model information)\n",
    "NB_SELECTED_IMAGES_PER_VIDEO = model.input_shape[1]\n",
    "PATCH_WIDTH = model.input_shape[2]\n",
    "PATCH_HEIGHT = model.input_shape[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. LOAD DATA\n",
    "imgs = np.zeros(shape=(NB_SELECTED_IMAGES_PER_VIDEO, IMAGE_HEIGHT, IMAGE_WIDTH, 1))\n",
    "\n",
    "# load images (imgs contains the whole video)\n",
    "for j in range(NB_SELECTED_IMAGES_PER_VIDEO):\n",
    "\n",
    "    if (IMAGE_CHANNELS==3):\n",
    "        temp = video.faces[j]/255\n",
    "        temp = temp[:,:,1]      # only the G component is currently used\n",
    "    else:\n",
    "        temp = video.faces[j] / 255\n",
    "\n",
    "    imgs[j] = np.expand_dims(temp, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formating Video and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPM frequency estimated = 122.5\n"
     ]
    }
   ],
   "source": [
    "xtest = np.zeros(shape=(NB_SELECTED_IMAGES_PER_VIDEO, PATCH_WIDTH , PATCH_HEIGHT, 1))\n",
    "for j in range(0,NB_SELECTED_IMAGES_PER_VIDEO):\n",
    "    faceCopy = copy(imgs[j])\n",
    "    np.random.shuffle(faceCopy)\n",
    "    for m in range(0, PATCH_WIDTH):\n",
    "        for n in range(0, PATCH_HEIGHT):\n",
    "            xtest[j][m][n]= faceCopy[m][n]\n",
    "            \n",
    "            \n",
    "h = model.predict(np.expand_dims(xtest, 0))\n",
    "\n",
    "def idxMax(list):\n",
    "    idx =0\n",
    "    maxi =0\n",
    "    for i in range(0, len(list)):\n",
    "        if maxi < list[i]:\n",
    "            idx = i\n",
    "            maxi = list[i]\n",
    "    return idx\n",
    "\n",
    "print('BPM frequency estimated = ' + str(freq_BPM[idxMax(h[0])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
